version: '3.8'

# ==============================================================================
# miStudio - Full Stack Docker Compose
# ==============================================================================
# Usage:
#   Development:  docker-compose up -d
#   Production:   docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
#   Stop:         docker-compose down
#   With logs:    docker-compose up
# ==============================================================================

services:
  # ============================================================================
  # Infrastructure Services
  # ============================================================================

  postgres:
    image: postgres:14-alpine
    container_name: mistudio-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-devpassword}
      POSTGRES_DB: ${POSTGRES_DB:-mistudio}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - mistudio-network

  # ============================================================================
  # Neuronpedia PostgreSQL (Separate Database)
  # NOTE: Uses pgvector image for vector extension required by Neuronpedia
  # ============================================================================
  neuronpedia-postgres:
    image: pgvector/pgvector:pg15
    container_name: mistudio-neuronpedia-postgres
    environment:
      POSTGRES_USER: neuronpedia
      POSTGRES_PASSWORD: neuronpedia_local_dev
      POSTGRES_DB: neuronpedia
    ports:
      - "5433:5432"  # Use different host port to avoid conflict
    volumes:
      - neuronpedia_postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U neuronpedia"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - mistudio-network

  redis:
    image: redis:7-alpine
    container_name: mistudio-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - mistudio-network

  # ============================================================================
  # Backend Services
  # ============================================================================

  backend:
    image: onegaionegai/mistudio-backend:latest
    container_name: mistudio-backend
    environment:
      SERVICE_TYPE: api
      RUN_MIGRATIONS: "true"
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-devpassword}@postgres:5432/${POSTGRES_DB:-mistudio}
      DATABASE_URL_SYNC: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-devpassword}@postgres:5432/${POSTGRES_DB:-mistudio}
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      DATA_DIR: /data
      BACKEND_DIR: /app
      HF_HOME: /data/huggingface_cache
      HF_CACHE_DIR: /data/huggingface_cache
      HF_TOKEN: ${HF_TOKEN:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      API_HOST: 0.0.0.0
      API_PORT: 8000
      API_BASE_URL: http://dev-mistudio.mcslab.io
      ALLOWED_ORIGINS: '["http://dev-mistudio.mcslab.io","http://localhost:3000","http://localhost"]'
      SECRET_KEY: ${SECRET_KEY:-change-me-in-production-use-a-long-random-string-here}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ENVIRONMENT: ${ENVIRONMENT:-development}
      DEBUG: ${DEBUG:-true}
      # Cross-deployment URLs (use Docker service names)
      INTERNAL_API_URL: http://backend:8000
      OLLAMA_URL: http://ollm:11434
      # Neuronpedia local instance configuration
      NEURONPEDIA_LOCAL_DB_URL: postgresql://neuronpedia:neuronpedia_local_dev@neuronpedia-postgres:5432/neuronpedia
      NEURONPEDIA_LOCAL_URL: http://localhost:3001
    ports:
      - "8000:8000"
    volumes:
      - ./backend/data:/data  # Mount host data directory for SAEs, models, etc.
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      neuronpedia-postgres:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/system/health"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    networks:
      - mistudio-network

  celery-worker:
    image: onegaionegai/mistudio-backend:latest
    container_name: mistudio-celery-worker
    healthcheck:
      disable: true  # Celery workers don't have HTTP endpoints
    environment:
      SERVICE_TYPE: celery-worker
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-devpassword}@postgres:5432/${POSTGRES_DB:-mistudio}
      DATABASE_URL_SYNC: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-devpassword}@postgres:5432/${POSTGRES_DB:-mistudio}
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      CELERY_QUEUES: high_priority,datasets,processing,training,extraction,sae,low_priority
      CELERY_CONCURRENCY: 1
      CELERY_MAX_TASKS: 5
      DATA_DIR: /data
      BACKEND_DIR: /app
      HF_HOME: /data/huggingface_cache
      HF_CACHE_DIR: /data/huggingface_cache
      HF_TOKEN: ${HF_TOKEN:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      SECRET_KEY: ${SECRET_KEY:-change-me-in-production-use-a-long-random-string-here}
      ALLOWED_ORIGINS: '["http://dev-mistudio.mcslab.io","http://localhost:3000","http://localhost"]'
      API_BASE_URL: http://dev-mistudio.mcslab.io
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      # Cross-deployment URLs (use Docker service names)
      INTERNAL_API_URL: http://backend:8000
      OLLAMA_URL: http://ollm:11434
      # Neuronpedia local instance configuration
      NEURONPEDIA_LOCAL_DB_URL: postgresql://neuronpedia:neuronpedia_local_dev@neuronpedia-postgres:5432/neuronpedia
      NEURONPEDIA_LOCAL_URL: http://localhost:3001
    volumes:
      - ./backend/data:/data  # Mount host data directory for SAEs, models, etc.
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      neuronpedia-postgres:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - mistudio-network

  celery-beat:
    image: onegaionegai/mistudio-backend:latest
    container_name: mistudio-celery-beat
    healthcheck:
      disable: true  # Celery beat doesn't have HTTP endpoints
    environment:
      SERVICE_TYPE: celery-beat
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-devpassword}@postgres:5432/${POSTGRES_DB:-mistudio}
      DATABASE_URL_SYNC: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-devpassword}@postgres:5432/${POSTGRES_DB:-mistudio}
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      SECRET_KEY: ${SECRET_KEY:-change-me-in-production-use-a-long-random-string-here}
      ALLOWED_ORIGINS: '["http://dev-mistudio.mcslab.io","http://localhost:3000","http://localhost"]'
      API_BASE_URL: http://dev-mistudio.mcslab.io
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      DATA_DIR: /data
      BACKEND_DIR: /app
      # Cross-deployment URLs (use Docker service names)
      INTERNAL_API_URL: http://backend:8000
      OLLAMA_URL: http://ollm:11434
      # Neuronpedia local instance configuration (for consistency)
      NEURONPEDIA_LOCAL_DB_URL: postgresql://neuronpedia:neuronpedia_local_dev@neuronpedia-postgres:5432/neuronpedia
      NEURONPEDIA_LOCAL_URL: http://localhost:3001
    restart: unless-stopped
    depends_on:
      - celery-worker
    networks:
      - mistudio-network

  # ============================================================================
  # Frontend Service
  # ============================================================================

  frontend:
    image: onegaionegai/mistudio-frontend:latest
    container_name: mistudio-frontend
    environment:
      NODE_ENV: production
      VITE_API_URL: ""
      VITE_WS_URL: ""
    ports:
      - "3000:80"  # Frontend image serves on port 80
    restart: unless-stopped
    networks:
      - mistudio-network

  # ============================================================================
  # Reverse Proxy
  # ============================================================================

  nginx:
    image: nginx:alpine
    container_name: mistudio-nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.docker.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped
    depends_on:
      - backend
      - frontend
    networks:
      - mistudio-network

  # ============================================================================
  # AI/LLM Service (oLLM - Memory-Efficient Inference)
  # ============================================================================

  ollm:
    build:
      context: ./backend/services/ollm_server
      dockerfile: Dockerfile
    container_name: mistudio-ollm
    ports:
      - "11434:11434"
    volumes:
      - ./backend/data:/data  # Shared data directory for models and cache
    environment:
      OLLM_HOST: 0.0.0.0
      OLLM_PORT: 11434
      OLLM_DEFAULT_MODEL: ${OLLM_DEFAULT_MODEL:-TinyLlama/TinyLlama-1.1B-Chat-v1.0}
      OLLM_MODEL_CACHE_DIR: /data/ollm_models
      OLLM_SSD_CACHE_DIR: /data/ollm_cache
      OLLM_ENABLE_SSD_OFFLOAD: "true"
      OLLM_MAX_CONTEXT_LENGTH: 8192
      OLLM_LOG_LEVEL: INFO
      HF_TOKEN: ${HF_TOKEN:-}
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s  # Models take time to load
      retries: 3
    networks:
      - mistudio-network

  # ============================================================================
  # Neuronpedia Webapp (Feature Dashboard)
  # ============================================================================
  neuronpedia:
    image: hitsai/neuronpedia-webapp:latest
    container_name: mistudio-neuronpedia
    environment:
      # Database connection - use Docker service name
      POSTGRES_PRISMA_URL: postgresql://neuronpedia:neuronpedia_local_dev@neuronpedia-postgres:5432/neuronpedia?pgbouncer=true
      POSTGRES_URL_NON_POOLING: postgresql://neuronpedia:neuronpedia_local_dev@neuronpedia-postgres:5432/neuronpedia
      DATABASE_URL: postgresql://neuronpedia:neuronpedia_local_dev@neuronpedia-postgres:5432/neuronpedia
      # Required environment variables
      NEXTAUTH_SECRET: neuronpedia-dev-secret-change-in-production
      NEXTAUTH_URL: http://localhost:3001
      # Local mode settings (these are build-time but we set them anyway for clarity)
      NEXT_PUBLIC_DEMO_MODE: "false"
      NEXT_PUBLIC_CONTACT_EMAIL_ADDRESS: admin@localhost
      # Secrets for local inference
      INFERENCE_SERVER_SECRET: local-secret
      AUTOINTERPRET_SERVER_SECRET: local-secret
    ports:
      - "3001:3000"  # Neuronpedia runs on port 3000 inside container
    restart: unless-stopped
    depends_on:
      neuronpedia-postgres:
        condition: service_healthy
    networks:
      - mistudio-network

# ==============================================================================
# Volumes
# ==============================================================================
volumes:
  postgres_data:
    name: mistudio_postgres_data
  redis_data:
    name: mistudio_redis_data
  mistudio_data:
    name: mistudio_data
  neuronpedia_postgres_data:
    name: mistudio_neuronpedia_postgres_data
  # Note: ollama_data volume removed - oLLM uses ./backend/data for model storage

# ==============================================================================
# Networks
# ==============================================================================
networks:
  mistudio-network:
    name: mistudio-network
    driver: bridge
