# Deep Code Dive: Tokenization Filtering & Multiple Tokenizations

**Date:** 2025-11-11
**Purpose:** Verify strict filtering + remove_all_punctuation works and confirm multiple tokenizations per dataset

---

## Executive Summary

### ‚úÖ What's Working

1. **Filter Configuration Loading** - Correctly loads from database (per-tokenization)
2. **Filter Mode (Strict)** - Working correctly, filtering punctuation tokens
3. **remove_all_punctuation Parameter** - Properly threaded through entire codebase
4. **custom_filter_chars Parameter** - Properly threaded through entire codebase
5. **Token Filtering Logic** - Correctly implemented in TokenFilter class

### ‚ùå Critical Issue Found

**UNIQUE CONSTRAINT prevents multiple tokenizations for same dataset+model:**
```sql
"uq_dataset_model_tokenization" UNIQUE CONSTRAINT, btree (dataset_id, model_id)
```

**Impact:** User cannot create:
- Tokenization 1: WITH punctuation (for SAE training)
- Tokenization 2: WITHOUT punctuation (for semantic feature discovery)

Both would use same dataset and model, but different `remove_all_punctuation` settings.

---

## Code Flow Analysis

### 1. Configuration Flow (Dataset ‚Üí Tokenization ‚Üí Worker ‚Üí Service ‚Üí Filter)

#### Step 1.1: Database Schema
**File:** Database migration
**Tables:**
- `datasets` table has dataset-level filter settings (for backward compatibility)
- `dataset_tokenizations` table has per-tokenization settings

**Per-Tokenization Settings:**
```sql
remove_all_punctuation | boolean | not null | false
custom_filter_chars    | varchar(255) | null |
```

**Constraint:**
```sql
"uq_dataset_model_tokenization" UNIQUE CONSTRAINT, btree (dataset_id, model_id)
```

#### Step 1.2: Configuration Loading in Celery Task
**File:** [backend/src/workers/dataset_tasks.py:523-548](backend/src/workers/dataset_tasks.py#L523-L548)

```python
# Load filter configuration from tokenization object (per-job config)
tokenization_obj = db.query(DatasetTokenization).filter_by(id=tokenization_id).first()
if tokenization_obj:
    filter_enabled = dataset_obj.tokenization_filter_enabled  # From dataset
    filter_mode = dataset_obj.tokenization_filter_mode
    filter_threshold = dataset_obj.tokenization_junk_ratio_threshold
    # Load new per-tokenization filter settings
    remove_all_punctuation = tokenization_obj.remove_all_punctuation  # ‚úÖ From tokenization
    custom_filter_chars = tokenization_obj.custom_filter_chars        # ‚úÖ From tokenization
else:
    # Fallback to dataset-level settings
    filter_enabled = dataset_obj.tokenization_filter_enabled
    filter_mode = dataset_obj.tokenization_filter_mode
    filter_threshold = dataset_obj.tokenization_junk_ratio_threshold
    remove_all_punctuation = False
    custom_filter_chars = None

# DEBUG: Log the loaded filter configuration values
logger.info(
    f"[FILTER_CONFIG] Loaded from DB - "
    f"filter_enabled={filter_enabled} (type={type(filter_enabled).__name__}), "
    f"filter_mode={filter_mode}, "
    f"filter_threshold={filter_threshold}, "
    f"remove_all_punctuation={remove_all_punctuation}, "
    f"custom_filter_chars={custom_filter_chars}"
)
```

**Current Job Log:**
```
[FILTER_CONFIG] Loaded from DB - filter_enabled=True (type=bool), filter_mode=strict,
filter_threshold=0.5, remove_all_punctuation=False, custom_filter_chars=None
```

‚úÖ **Confirmed:** Per-tokenization settings are loaded correctly

#### Step 1.3: Passing to Tokenization Service
**File:** [backend/src/workers/dataset_tasks.py:714-735](backend/src/workers/dataset_tasks.py#L714-L735)

```python
# DEBUG: Log values being passed to tokenizer
logger.info(
    f"[FILTER_CONFIG] Passing to tokenizer - "
    f"enable_filtering={filter_enabled} (type={type(filter_enabled).__name__}), "
    f"filter_mode={filter_mode}, "
    f"junk_ratio_threshold={filter_threshold}"
)

try:
    tokenized_dataset = TokenizationService.tokenize_dataset(
        dataset=dataset,
        tokenizer=tokenizer,
        text_column=text_column,
        max_length=max_length,
        stride=stride,
        truncation=truncation_param,
        padding=padding,
        add_special_tokens=add_special_tokens,
        return_attention_mask=return_attention_mask,
        enable_cleaning=enable_cleaning,
        batch_size=1000,
        progress_callback=None,
        num_proc=None,
        # Use filter config from tokenization (per-job) instead of global settings
        enable_filtering=filter_enabled,
        filter_mode=filter_mode,
        junk_ratio_threshold=filter_threshold,
        remove_all_punctuation=remove_all_punctuation,  # ‚úÖ Passed
        custom_filter_chars=custom_filter_chars,        # ‚úÖ Passed
    )
```

‚úÖ **Confirmed:** Parameters correctly passed to service

#### Step 1.4: Tokenization Service (Multiprocessing Branch)
**File:** [backend/src/services/tokenization_service.py:427-443](backend/src/services/tokenization_service.py#L427-L443)

```python
mapper = _TokenizationMapper(
    tokenizer_name=tokenizer_name,
    text_column=text_column,
    max_length=max_length,
    truncation=truncation,
    padding=padding,
    add_special_tokens=add_special_tokens,
    return_attention_mask=return_attention_mask,
    stride=stride,
    return_overflowing_tokens=return_overflowing_tokens,
    enable_cleaning=enable_cleaning,
    enable_filtering=enable_filtering,
    filter_mode=filter_mode,
    junk_ratio_threshold=junk_ratio_threshold,
    remove_all_punctuation=remove_all_punctuation,  # ‚úÖ Passed to mapper
    custom_filter_chars=custom_filter_chars,        # ‚úÖ Passed to mapper
)
```

‚úÖ **Confirmed:** Parameters correctly passed to mapper

#### Step 1.5: TokenizationMapper Initialization
**File:** [backend/src/services/tokenization_service.py:29-65](backend/src/services/tokenization_service.py#L29-L65)

```python
def __init__(
    self,
    tokenizer_name: str,
    text_column: str,
    max_length: int,
    truncation: bool,
    padding: str,
    add_special_tokens: bool,
    return_attention_mask: bool,
    stride: int = 0,
    return_overflowing_tokens: bool = False,
    enable_cleaning: bool = True,
    enable_filtering: bool = False,
    filter_mode: str = "conservative",
    junk_ratio_threshold: float = 0.7,
    remove_all_punctuation: bool = False,  # ‚úÖ Parameter defined
    custom_filter_chars: Optional[str] = None,  # ‚úÖ Parameter defined
):
    """Initialize the tokenization mapper with parameters."""
    self.tokenizer_name = tokenizer_name
    self.text_column = text_column
    self.max_length = max_length
    self.truncation = truncation
    self.padding = padding
    self.add_special_tokens = add_special_tokens
    self.return_attention_mask = return_attention_mask
    self.stride = stride
    self.return_overflowing_tokens = return_overflowing_tokens
    self.enable_cleaning = enable_cleaning
    self.enable_filtering = enable_filtering
    self.filter_mode = filter_mode
    self.junk_ratio_threshold = junk_ratio_threshold
    self.remove_all_punctuation = remove_all_punctuation  # ‚úÖ Stored
    self.custom_filter_chars = custom_filter_chars        # ‚úÖ Stored
    self._tokenizer = None  # Lazy-loaded in worker process
    self._text_cleaner = None  # Lazy-loaded in worker process
    self._token_filter = None  # Lazy-loaded in worker process
```

‚úÖ **Confirmed:** Parameters stored in mapper instance

#### Step 1.6: TokenFilter Instantiation
**File:** [backend/src/services/tokenization_service.py:85-95](backend/src/services/tokenization_service.py#L85-L95)

```python
def _get_token_filter(self):
    """Lazy-load token filter in worker process (avoids pickling issues)."""
    if self._token_filter is None:
        from ..utils.token_filter import TokenFilter, FilterMode
        mode = FilterMode[self.filter_mode.upper()]
        self._token_filter = TokenFilter(
            mode=mode,
            remove_all_punctuation=self.remove_all_punctuation,  # ‚úÖ Passed to filter
            custom_filter_chars=self.custom_filter_chars          # ‚úÖ Passed to filter
        )
    return self._token_filter
```

‚úÖ **Confirmed:** Parameters correctly passed to TokenFilter

---

### 2. TokenFilter Implementation

#### Step 2.1: TokenFilter Initialization
**File:** [backend/src/utils/token_filter.py:32-59](backend/src/utils/token_filter.py#L32-L59)

```python
def __init__(
    self,
    mode: FilterMode = FilterMode.STANDARD,
    keep_patterns: Optional[List[str]] = None,
    custom_junk_tokens: Optional[Set[str]] = None,
    remove_all_punctuation: bool = False,         # ‚úÖ Parameter defined
    custom_filter_chars: Optional[str] = None     # ‚úÖ Parameter defined
):
    """
    Initialize token filter.

    Args:
        mode: Filter aggressiveness level
        keep_patterns: Regex patterns for tokens to always keep (e.g., r'C\+\+', r'\.NET')
        custom_junk_tokens: Additional tokens to always filter
        remove_all_punctuation: If True, removes ALL punctuation characters (overrides mode)
        custom_filter_chars: Additional characters to filter (e.g., "~@#$%")
    """
    self.mode = mode
    self.keep_patterns = keep_patterns or [
        r'C\+\+',  # Programming languages
        r'F#',
        r'C#',
        r'\.NET',
    ]
    self.custom_junk_tokens = custom_junk_tokens or set()
    self.remove_all_punctuation = remove_all_punctuation    # ‚úÖ Stored
    self.custom_filter_chars = set(custom_filter_chars) if custom_filter_chars else set()  # ‚úÖ Stored
```

‚úÖ **Confirmed:** Parameters stored in filter instance

#### Step 2.2: remove_all_punctuation Logic
**File:** [backend/src/utils/token_filter.py:105-137](backend/src/utils/token_filter.py#L105-L137)

```python
def _contains_any_punctuation(self, token: str) -> bool:
    """Check if token contains any punctuation characters."""
    cleaned = self._clean_bpe_markers(token)
    return any(c in string.punctuation for c in cleaned)

def is_junk_token(self, token: str) -> bool:
    """
    Determine if token should be filtered based on mode.

    Returns True if token should be filtered (is junk).
    """
    # Always keep tokens matching keep patterns
    if self._matches_keep_pattern(token):
        return False

    # Always filter custom junk tokens
    if token in self.custom_junk_tokens:
        return True

    # Filter tokens containing custom filter characters
    if self._contains_custom_chars(token):
        return True

    # If remove_all_punctuation is enabled, filter any token with punctuation
    if self.remove_all_punctuation and self._contains_any_punctuation(token):
        return True  # ‚úÖ Filters ANY token containing ANY punctuation

    # ... rest of mode-based filtering logic
```

**Logic Flow:**
1. Check if token matches keep patterns ‚Üí Keep it
2. Check if token in custom_junk_tokens ‚Üí Filter it
3. Check if token contains custom_filter_chars ‚Üí Filter it
4. **If `remove_all_punctuation=True` and token contains ANY punctuation ‚Üí Filter it**
5. Otherwise, apply mode-based filtering (MINIMAL, CONSERVATIVE, STANDARD, AGGRESSIVE, STRICT)

‚úÖ **Confirmed:** `remove_all_punctuation` correctly filters ANY token with ANY punctuation

#### Step 2.3: STRICT Mode Logic
**File:** [backend/src/utils/token_filter.py:191-200](backend/src/utils/token_filter.py#L191-L200)

```python
# STRICT mode: Filter ANY token containing punctuation
if self.mode == FilterMode.STRICT:
    if self._is_control_char(token) or self._is_whitespace_only(token):
        return True

    # Filter any token containing ANY punctuation
    if self._contains_any_punctuation(token):
        return True

    return False
```

**STRICT Mode Behavior:**
- Filters control characters
- Filters whitespace-only tokens
- Filters ANY token containing ANY punctuation (even within words like "don't" ‚Üí filtered)

**Difference between STRICT and remove_all_punctuation:**
- **STRICT mode:** Built-in filter mode that removes tokens with punctuation
- **remove_all_punctuation:** Per-tokenization override that does the same thing, checked BEFORE mode logic

**Result:** When `remove_all_punctuation=True`, it behaves exactly like STRICT mode's punctuation filtering

‚úÖ **Confirmed:** STRICT mode working correctly

---

### 3. Verification from Current Job Logs

**Configuration Loaded:**
```
[FILTER_CONFIG] Loaded from DB - filter_enabled=True (type=bool), filter_mode=strict,
filter_threshold=0.5, remove_all_punctuation=False, custom_filter_chars=None
```

**Filtering Results:**
```
Tokenization filter: 1000 samples kept, 0 junk samples filtered (0.0% filtered)
Tokenization filter: 1000 samples kept, 0 junk samples filtered (0.0% filtered)
...
```

**Analysis:**
- ‚úÖ `filter_enabled=True` - Filtering is enabled
- ‚úÖ `filter_mode=strict` - Using STRICT mode
- ‚úÖ `remove_all_punctuation=False` - NOT removing all punctuation (current job)
- ‚úÖ `0.0% filtered` - Expected for openwebtext with strict filtering (high-quality dataset)

**Explanation:** OpenWebText is a curated, high-quality dataset derived from web pages. With STRICT filtering, very few samples have >50% junk tokens (threshold=0.5), so 0% filtering is expected.

---

## Critical Issue: UNIQUE CONSTRAINT

### Problem

**Database Constraint:**
```sql
"uq_dataset_model_tokenization" UNIQUE CONSTRAINT, btree (dataset_id, model_id)
```

**Impact:**
Cannot create multiple tokenizations for the same (dataset, model) pair, even with different settings.

### User's Use Case

**Scenario:** User wants two tokenizations for the same dataset:

1. **Tokenization A (SAE Training):**
   - `remove_all_punctuation=False`
   - Keep punctuation for training Sparse Autoencoders
   - Learn features that include punctuation patterns

2. **Tokenization B (Semantic Feature Discovery):**
   - `remove_all_punctuation=True`
   - Remove ALL punctuation
   - Focus on semantic meaning without punctuation noise

**Current State:** ‚ùå BLOCKED by unique constraint

### Solution Options

#### Option 1: Remove UNIQUE Constraint (Recommended)

**Change:** Remove the `uq_dataset_model_tokenization` constraint

**Pros:**
- Allows multiple tokenizations per (dataset, model) with different settings
- Maximum flexibility for experimentation
- Aligns with user's use case

**Cons:**
- Could lead to many tokenizations for same dataset (storage overhead)
- Need UI updates to distinguish tokenizations clearly

**Implementation:**
```sql
-- Migration to remove constraint
ALTER TABLE dataset_tokenizations DROP CONSTRAINT uq_dataset_model_tokenization;
```

**UI Changes Needed:**
- Display tokenization settings in UI (remove_all_punctuation, custom_filter_chars)
- Allow users to distinguish between multiple tokenizations
- Add "Tokenization Name" field for user-friendly identification

#### Option 2: Add Settings to Composite Key

**Change:** Make the unique constraint include filter settings

**New Constraint:**
```sql
UNIQUE (dataset_id, model_id, remove_all_punctuation, custom_filter_chars)
```

**Pros:**
- Prevents duplicate tokenizations with identical settings
- Still allows multiple tokenizations with different settings

**Cons:**
- Complex constraint
- NULL handling for custom_filter_chars
- Still limits flexibility (what if threshold changes?)

#### Option 3: Add Tokenization "Profile" or "Name"

**Change:** Add a `name` or `profile` field to make each tokenization unique

**Example:**
```sql
ALTER TABLE dataset_tokenizations ADD COLUMN name VARCHAR(255) NOT NULL DEFAULT '';
ALTER TABLE dataset_tokenizations DROP CONSTRAINT uq_dataset_model_tokenization;
ALTER TABLE dataset_tokenizations ADD CONSTRAINT uq_dataset_model_name UNIQUE (dataset_id, model_id, name);
```

**Pros:**
- User-friendly naming
- Clear distinction between tokenizations
- Prevents accidental duplicates

**Cons:**
- Requires UI for naming
- Default name handling

---

## Recommendations

### Short Term (Immediate Fix)

1. **Remove UNIQUE Constraint:**
   ```sql
   ALTER TABLE dataset_tokenizations DROP CONSTRAINT uq_dataset_model_tokenization;
   ```

2. **Update UI to Display Settings:**
   - Show `remove_all_punctuation` badge on tokenization cards
   - Show `custom_filter_chars` if present
   - Add tooltip explaining each setting

### Medium Term (Better UX)

1. **Add Tokenization Name Field:**
   - Allow users to name tokenizations (e.g., "With Punctuation", "Semantic Only")
   - Auto-generate name based on settings if not provided
   - Update UI to show name prominently

2. **Add Validation:**
   - Warn user if creating duplicate tokenization with identical settings
   - Allow override if user confirms

### Long Term (Feature Enhancement)

1. **Tokenization Profiles:**
   - Predefined profiles (e.g., "SAE Training", "Semantic Analysis", "Full Fidelity")
   - Save custom profiles for reuse
   - Share profiles across datasets

---

## Testing Checklist

### Current Status

- ‚úÖ Filter configuration loading from database
- ‚úÖ `remove_all_punctuation` parameter threading
- ‚úÖ `custom_filter_chars` parameter threading
- ‚úÖ STRICT mode filtering logic
- ‚úÖ `remove_all_punctuation=True` filtering logic
- ‚úÖ Filtering statistics logging
- ‚ùå **Multiple tokenizations blocked by UNIQUE constraint**

### Required Testing (After Constraint Removal)

1. **Test Multiple Tokenizations:**
   - [ ] Create tokenization with `remove_all_punctuation=False`
   - [ ] Create second tokenization with `remove_all_punctuation=True` for same dataset
   - [ ] Verify both tokenizations complete successfully
   - [ ] Verify different token counts / statistics

2. **Test remove_all_punctuation=True:**
   - [ ] Create tokenization with `remove_all_punctuation=True`
   - [ ] Verify log shows filtering > 0% (should filter punctuation tokens)
   - [ ] Compare vocab_size to tokenization with `remove_all_punctuation=False`
   - [ ] Expected: Smaller vocab_size (punctuation tokens removed)

3. **Test custom_filter_chars:**
   - [ ] Create tokenization with `custom_filter_chars="~@#$%"`
   - [ ] Verify tokens containing these characters are filtered
   - [ ] Check filtering statistics

4. **Test STRICT Mode vs remove_all_punctuation:**
   - [ ] Create tokenization: `mode=STRICT, remove_all_punctuation=False`
   - [ ] Create tokenization: `mode=CONSERVATIVE, remove_all_punctuation=True`
   - [ ] Verify both produce similar results (both filter punctuation)

---

## Conclusion

**Summary:**
- ‚úÖ Filter configuration and parameters are correctly threaded through entire codebase
- ‚úÖ STRICT mode and `remove_all_punctuation` logic are correctly implemented
- ‚ùå **CRITICAL:** UNIQUE constraint blocks multiple tokenizations per (dataset, model)
- üéØ **Action Required:** Remove UNIQUE constraint to enable user's use case

**Next Steps:**
1. Remove `uq_dataset_model_tokenization` constraint via database migration
2. Update UI to display tokenization settings clearly
3. Test multiple tokenizations with different settings
4. Consider adding tokenization "name" field for better UX

---

**End of Deep Dive**
